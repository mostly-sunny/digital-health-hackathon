{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coxph-find-best-network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostly-sunny/digital-health-hackathon/blob/main/3.%20coxph_find_best_network_by_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L91UX-HYY5Qr"
      },
      "source": [
        "# Pycox - CoxPH Model\n",
        "- Network : Test with sets, and find best network and lr by lowest brier score\n",
        "- Input Variables : G1 ~ G300, Var1 ~ Var10, Treatment\n",
        "- Output Variables : time, event\n",
        "- Scaler : MinMaxScaler -> Var1 ~ Var10\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlcJyLq91wdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cef0200-3576-4192-ccc8-06c5e3ddeb5b"
      },
      "source": [
        "pip install pycox"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycox\n",
            "  Downloading pycox-0.2.2-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting py7zr>=0.11.3\n",
            "  Downloading py7zr-0.16.1-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from pycox) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.44 in /usr/local/lib/python3.7/dist-packages (from pycox) (0.51.2)\n",
            "Collecting torchtuples>=0.2.0\n",
            "  Downloading torchtuples-0.2.2-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 699 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.7/dist-packages (from pycox) (0.22.2.post1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pycox) (2.23.0)\n",
            "Requirement already satisfied: feather-format>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pycox) (0.4.1)\n",
            "Requirement already satisfied: pyarrow>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from feather-format>=0.4.0->pycox) (3.0.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->pycox) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->pycox) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.44->pycox) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.44->pycox) (0.34.0)\n",
            "Collecting bcj-cffi<0.6.0,>=0.5.1\n",
            "  Downloading bcj_cffi-0.5.1-cp37-cp37m-manylinux2014_x86_64.whl (36 kB)\n",
            "Collecting pycryptodomex>=3.6.6\n",
            "  Downloading pycryptodomex-3.10.4-cp35-abi3-manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 18.0 MB/s \n",
            "\u001b[?25hCollecting pyppmd>=0.14.0\n",
            "  Downloading pyppmd-0.17.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 53.2 MB/s \n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from py7zr>=0.11.3->pycox) (4.8.1)\n",
            "Collecting pyzstd<0.15.0,>=0.14.4\n",
            "  Downloading pyzstd-0.14.4-cp37-cp37m-manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 29.6 MB/s \n",
            "\u001b[?25hCollecting brotli>=1.0.9\n",
            "  Downloading Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357 kB)\n",
            "\u001b[K     |████████████████████████████████| 357 kB 39.2 MB/s \n",
            "\u001b[?25hCollecting texttable\n",
            "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: cffi>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from bcj-cffi<0.6.0,>=0.5.1->py7zr>=0.11.3->pycox) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.14.0->bcj-cffi<0.6.0,>=0.5.1->py7zr>=0.11.3->pycox) (2.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pycox) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pycox) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pycox) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pycox) (2021.5.30)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->pycox) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->pycox) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from torchtuples>=0.2.0->pycox) (1.1.5)\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from torchtuples>=0.2.0->pycox) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->torchtuples>=0.2.0->pycox) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->py7zr>=0.11.3->pycox) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->py7zr>=0.11.3->pycox) (3.7.4.3)\n",
            "Installing collected packages: texttable, pyzstd, pyppmd, pycryptodomex, multivolumefile, brotli, bcj-cffi, torchtuples, py7zr, pycox\n",
            "Successfully installed bcj-cffi-0.5.1 brotli-1.0.9 multivolumefile-0.2.3 py7zr-0.16.1 pycox-0.2.2 pycryptodomex-3.10.4 pyppmd-0.17.0 pyzstd-0.14.4 texttable-1.6.4 torchtuples-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j42Q8682S-J"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn_pandas import DataFrameMapper\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchtuples as tt\n",
        "\n",
        "from pycox.models import CoxPH\n",
        "from pycox.evaluation import EvalSurv"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgR8rJ3epF44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01cfe5eb-febd-4a42-ce14-925c9ef7e491"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgHuZOMr6wdT"
      },
      "source": [
        "np.random.seed(123456)\n",
        "_ = torch.manual_seed(123456)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOyFOsuvkXqN"
      },
      "source": [
        "- all-in-one.csv 파일은 유전자 변이 유무, 임상 변수, 생존 기간, 사망 여부, 치료 유무가 열로 존재하는 파일\n",
        "- test-data-treat-and-untreat.csv 파일은 all-in-one의 열은 같은 602개의 데이터.\n",
        "  - (0번째 행) : 유전자 변이 모두 0, 치료 0\n",
        "  - (1번째 행) : 유전자 변이 모두 0, 치료 1\n",
        "  - (2~301번째 행) : 유전자 변이 n-1에만 1, 치료 0\n",
        "  - (302~601번재 행) : 유전자 변이 n-301에만 1, 치료 1\n",
        "- pandas 라이브러리에 있는 csv 파일을 DataFrame으로 바꾸어주는 read_csv 함수를 이용하여 파일을 읽어 들임.\n",
        "- DataFrame은 표를 나타내는 데이터 타입임."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Ww7lpV2Uor"
      },
      "source": [
        "dataset = pd.read_csv('/content/all-in-one.csv')\n",
        "dataset_for_hr = pd.read_csv('/content/test-data-treat-and-untreat.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twdq_J1Ym9gf"
      },
      "source": [
        "- 위에서 읽어들인 dataset 중에서 20%는 검증(_val -> validation)을 위해 sampling 한다.\n",
        "- 남은 80%의 데이터에서도 20%는 테스트(_test)를 위해 sampling 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4vLVwYM24xW"
      },
      "source": [
        "dataset_val = dataset.sample(frac=0.2, random_state = np.random.seed(123456))\n",
        "dataset_train = dataset.drop(dataset_val.index)\n",
        "dataset_test = dataset_train.sample(frac=0.2, random_state = np.random.seed(123456))\n",
        "dataset_train = dataset_train.drop(dataset_test.index)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXu0kxYzRuc-",
        "outputId": "0792a8df-071f-4057-998e-e94e3ec74314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "dataset_val"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>G1</th>\n",
              "      <th>G2</th>\n",
              "      <th>G3</th>\n",
              "      <th>G4</th>\n",
              "      <th>G5</th>\n",
              "      <th>G6</th>\n",
              "      <th>G7</th>\n",
              "      <th>G8</th>\n",
              "      <th>G9</th>\n",
              "      <th>G10</th>\n",
              "      <th>G11</th>\n",
              "      <th>G12</th>\n",
              "      <th>G13</th>\n",
              "      <th>G14</th>\n",
              "      <th>G15</th>\n",
              "      <th>G16</th>\n",
              "      <th>G17</th>\n",
              "      <th>G18</th>\n",
              "      <th>G19</th>\n",
              "      <th>G20</th>\n",
              "      <th>G21</th>\n",
              "      <th>G22</th>\n",
              "      <th>G23</th>\n",
              "      <th>G24</th>\n",
              "      <th>G25</th>\n",
              "      <th>G26</th>\n",
              "      <th>G27</th>\n",
              "      <th>G28</th>\n",
              "      <th>G29</th>\n",
              "      <th>G30</th>\n",
              "      <th>G31</th>\n",
              "      <th>G32</th>\n",
              "      <th>G33</th>\n",
              "      <th>G34</th>\n",
              "      <th>G35</th>\n",
              "      <th>G36</th>\n",
              "      <th>G37</th>\n",
              "      <th>G38</th>\n",
              "      <th>G39</th>\n",
              "      <th>G40</th>\n",
              "      <th>...</th>\n",
              "      <th>G274</th>\n",
              "      <th>G275</th>\n",
              "      <th>G276</th>\n",
              "      <th>G277</th>\n",
              "      <th>G278</th>\n",
              "      <th>G279</th>\n",
              "      <th>G280</th>\n",
              "      <th>G281</th>\n",
              "      <th>G282</th>\n",
              "      <th>G283</th>\n",
              "      <th>G284</th>\n",
              "      <th>G285</th>\n",
              "      <th>G286</th>\n",
              "      <th>G287</th>\n",
              "      <th>G288</th>\n",
              "      <th>G289</th>\n",
              "      <th>G290</th>\n",
              "      <th>G291</th>\n",
              "      <th>G292</th>\n",
              "      <th>G293</th>\n",
              "      <th>G294</th>\n",
              "      <th>G295</th>\n",
              "      <th>G296</th>\n",
              "      <th>G297</th>\n",
              "      <th>G298</th>\n",
              "      <th>G299</th>\n",
              "      <th>G300</th>\n",
              "      <th>Var1</th>\n",
              "      <th>Var2</th>\n",
              "      <th>Var3</th>\n",
              "      <th>Var4</th>\n",
              "      <th>Var5</th>\n",
              "      <th>Var6</th>\n",
              "      <th>Var7</th>\n",
              "      <th>Var8</th>\n",
              "      <th>Var9</th>\n",
              "      <th>Var10</th>\n",
              "      <th>time</th>\n",
              "      <th>event</th>\n",
              "      <th>Treatment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>601</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>71.955766</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>39.587305</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>945</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>99.459796</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>828</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>67.195341</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>35.031129</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>49.549464</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>642</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>43.761084</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>838</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>63.350415</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>42.715412</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>62.844956</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 313 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     G1  G2  G3  G4  G5  G6  ...  Var8  Var9  Var10       time  event  Treatment\n",
              "601   0   0   0   0   0   0  ...     5     7      1  71.955766      1          1\n",
              "357   0   0   0   0   0   0  ...     0     2      1  39.587305      1          0\n",
              "945   1   0   0   0   0   0  ...     2     2      3  99.459796      1          1\n",
              "828   0   0   0   0   0   0  ...     3     1      2  67.195341      1          1\n",
              "649   1   0   0   0   0   0  ...     1     0      6  35.031129      1          0\n",
              "..   ..  ..  ..  ..  ..  ..  ...   ...   ...    ...        ...    ...        ...\n",
              "295   0   1   0   0   0   0  ...     4     4      1  49.549464      1          0\n",
              "642   0   1   0   0   0   0  ...     3     1      1  43.761084      1          1\n",
              "838   0   0   0   1   0   0  ...     3     4      6  63.350415      0          1\n",
              "570   0   0   0   0   0   0  ...     4     2      2  42.715412      1          1\n",
              "695   0   0   1   0   0   1  ...     3     2      1  62.844956      1          1\n",
              "\n",
              "[200 rows x 313 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX1Ddl463deJ"
      },
      "source": [
        "- columns_standardize : 임상변수 - 0~9사이의 값을 표준화\n",
        "- columns_leave : 유전자 변이 유무 + 치료 유무 - 0과 1로 표현돼 있기 때문에 표준화 필요 없음.\n",
        "- DataFrameMapper는 pandas DataFrame에서 원하는 열을 뽑아서 리스트로 만들어줌.\n",
        "- 리스트로 만들때 StandardScaler()가 포함된 열은 표준화를 시킨 뒤, 그리고 None이면 갖고 있는 값을 그대로 넣음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHBg3yDH3Xzf"
      },
      "source": [
        "columns_standardize = ['Var' + str(i) for i in range(1,11)]\n",
        "columns_leave = ['G' + str(i) for i in range(1,301)]\n",
        "columns_leave += ['Treatment']\n",
        "\n",
        "# standardize = [([col], StandardScaler()) for col in columns_standardize]\n",
        "standardize = [([col], MinMaxScaler()) for col in columns_standardize]\n",
        "\n",
        "leave = [(col, None) for col in columns_leave]\n",
        "\n",
        "x_mapper = DataFrameMapper(leave + standardize)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZHTUBBSmWTc"
      },
      "source": [
        "- 위에서 만든 DataFrameMapper로 DataFrame 중 x(입력) 데이터를 모델이 학습할 수 있게끔 리스트 형식으로 바꾸어 준다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4UrVswf3phZ"
      },
      "source": [
        "x_train = x_mapper.fit_transform(dataset_train).astype('float32')\n",
        "x_val = x_mapper.transform(dataset_val).astype('float32')\n",
        "x_test = x_mapper.transform(dataset_test).astype('float32')\n",
        "x_for_hr = x_mapper.transform(dataset_for_hr).astype('float32')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4PCVeJGmx21"
      },
      "source": [
        "- DataFrame (표)에서 Y(출력)데이터인 time(생존기간)과 event(사망여부)를 뽑아 출력 데이터를 추린다.\n",
        "- 검증(Validation)을 위한 입력-출력 세트 val을 만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGTF5V3y4Yot"
      },
      "source": [
        "get_target = lambda df: (df['time'].values, df['event'].values)\n",
        "y_train = get_target(dataset_train)\n",
        "y_val = get_target(dataset_val)\n",
        "\n",
        "durations_test, events_test = get_target(dataset_test)\n",
        "val = x_val, y_val"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-rPa9mWn1I4"
      },
      "source": [
        "함수 make_net : network을 생성해 리턴하는 함수\n",
        "- input과 output의 노드 수, 은닉층 수, 은닉층의 노드 수 설정 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eauwa8i6iGmN"
      },
      "source": [
        "def make_net(in_features, out_features, hidden, nodes):\n",
        "  if hidden == 1:\n",
        "    network =  torch.nn.Sequential(\n",
        "      torch.nn.Linear(in_features, nodes),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.BatchNorm1d(nodes),\n",
        "      torch.nn.Dropout(0.1),\n",
        "            \n",
        "      torch.nn.Linear(nodes, out_features)\n",
        "    )\n",
        "  elif hidden == 2:\n",
        "    network =  torch.nn.Sequential(\n",
        "      torch.nn.Linear(in_features, nodes),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.BatchNorm1d(nodes),\n",
        "      torch.nn.Dropout(0.1),\n",
        "\n",
        "      torch.nn.Linear(nodes, nodes),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.BatchNorm1d(nodes),\n",
        "      torch.nn.Dropout(0.1),\n",
        "            \n",
        "      torch.nn.Linear(nodes, out_features)\n",
        "    )\n",
        "  elif hidden == 3:\n",
        "    network =  torch.nn.Sequential(\n",
        "      torch.nn.Linear(in_features, nodes),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.BatchNorm1d(nodes),\n",
        "      torch.nn.Dropout(0.1),\n",
        "\n",
        "      torch.nn.Linear(nodes, nodes),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.BatchNorm1d(nodes),\n",
        "      torch.nn.Dropout(0.1),\n",
        "\n",
        "      torch.nn.Linear(nodes, nodes),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.BatchNorm1d(nodes),\n",
        "      torch.nn.Dropout(0.1),\n",
        "            \n",
        "      torch.nn.Linear(nodes, out_features)\n",
        "    )\n",
        "  elif hidden == 4:\n",
        "    network =  torch.nn.Sequential(\n",
        "      torch.nn.Linear(in_features, nodes),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.BatchNorm1d(nodes),\n",
        "      torch.nn.Dropout(0.1),\n",
        "\n",
        "      torch.nn.Linear(nodes, nodes),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.BatchNorm1d(nodes),\n",
        "      torch.nn.Dropout(0.1),\n",
        "\n",
        "      torch.nn.Linear(nodes, nodes),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.BatchNorm1d(nodes),\n",
        "      torch.nn.Dropout(0.1),\n",
        "      \n",
        "      torch.nn.Linear(nodes, nodes),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.BatchNorm1d(nodes),\n",
        "      torch.nn.Dropout(0.1),\n",
        "\n",
        "      torch.nn.Linear(nodes, out_features)\n",
        "    )\n",
        "  return network"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0BBkip4uOx3"
      },
      "source": [
        "- in_features : 입력데이터의 개수 (x_train.shape : 311 = 300(유전자) + 10(임상변수) + 1(치료유무))\n",
        "- out_features : 출력노드의 개수\n",
        "\n",
        "- hidden_layers : 은닉층 수를 가지고 있는 리스트\n",
        "- number_nodes : 은닉층에 있는 노드 수를 가지고 있는 리스트\n",
        "- learning_rates : 테스트할 학습률을 가지고 있는 리스트\n",
        "- brier_scores = brier score을 계산해 append"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJHZogmj5a-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c72254f-ffc7-4098-f7f0-89dd61538b86"
      },
      "source": [
        "in_features = x_train.shape[1]\n",
        "out_features = 1\n",
        "\n",
        "hidden_layers = [1,2]\n",
        "number_nodes = [2000, 3000]\n",
        "learning_rates = [0.0001, 0.001, 0]\n",
        "brier_scores = []\n",
        "\n",
        "total_num = len(hidden_layers) * len(number_nodes) * len(learning_rates)\n",
        "count = 1\n",
        "\n",
        "for i in hidden_layers:\n",
        "  for j in number_nodes:\n",
        "    for k in learning_rates:\n",
        "      print(count, '/' , total_num)\n",
        "      net = make_net(in_features, out_features, i, j)\n",
        "      model = CoxPH(net, tt.optim.Adam)\n",
        "      batch_size = 639\n",
        "\n",
        "      if k == 0:\n",
        "        lrfinder = model.lr_finder(x_train, y_train, batch_size, tolerance = 10)\n",
        "        model.optimizer.set_lr(lrfinder.get_best_lr())\n",
        "      else:\n",
        "        model.optimizer.set_lr(k)\n",
        "      \n",
        "      epochs = 512\n",
        "      callbacks = [tt.callbacks.EarlyStopping()]\n",
        "      verbose = True\n",
        "\n",
        "      %%time\n",
        "      model.fit(x_train, y_train, batch_size, epochs, callbacks, verbose, val_data=val, val_batch_size=batch_size)\n",
        "      _ = model.compute_baseline_hazards()\n",
        "      surv = model.predict_surv_df(x_test)\n",
        "\n",
        "      count += 1\n",
        "\n",
        "      # calculate ratio\n",
        "      log_partial_hazard = model.predict(x_for_hr)\n",
        "      partial_hazard = [np.exp(lph) for lph in log_partial_hazard]\n",
        "\n",
        "      treat_hr = []\n",
        "      # ratio with treated and untreated\n",
        "      for g in range(300):\n",
        "        treat_hr.append([partial_hazard[g+302]/partial_hazard[g+2],'G' + str(g+1)])\n",
        "      treat_hr.sort()\n",
        "\n",
        "      # evaluation\n",
        "      ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n",
        "      time_grid = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
        "      score = ev.integrated_brier_score(time_grid)\n",
        "\n",
        "      if k == 0:\n",
        "        brier_scores.append([score, i, j, lrfinder.get_best_lr(), treat_hr[:10]])\n",
        "      else:\n",
        "        brier_scores.append([score, i, j, k, treat_hr[:10]])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 / 12\n",
            "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
            "Wall time: 10.7 µs\n",
            "0:\t[0s / 0s],\t\ttrain_loss: 5.5708,\tval_loss: 4.2832\n",
            "1:\t[0s / 0s],\t\ttrain_loss: 5.5007,\tval_loss: 4.2825\n",
            "2:\t[0s / 0s],\t\ttrain_loss: 5.4480,\tval_loss: 4.2815\n",
            "3:\t[0s / 0s],\t\ttrain_loss: 5.3859,\tval_loss: 4.2805\n",
            "4:\t[0s / 0s],\t\ttrain_loss: 5.3485,\tval_loss: 4.2793\n",
            "5:\t[0s / 0s],\t\ttrain_loss: 5.2865,\tval_loss: 4.2781\n",
            "6:\t[0s / 0s],\t\ttrain_loss: 5.2485,\tval_loss: 4.2767\n",
            "7:\t[0s / 0s],\t\ttrain_loss: 5.2143,\tval_loss: 4.2752\n",
            "8:\t[0s / 0s],\t\ttrain_loss: 5.1714,\tval_loss: 4.2736\n",
            "9:\t[0s / 0s],\t\ttrain_loss: 5.1383,\tval_loss: 4.2718\n",
            "10:\t[0s / 0s],\t\ttrain_loss: 5.0919,\tval_loss: 4.2699\n",
            "11:\t[0s / 0s],\t\ttrain_loss: 5.0752,\tval_loss: 4.2679\n",
            "12:\t[0s / 0s],\t\ttrain_loss: 5.0217,\tval_loss: 4.2659\n",
            "13:\t[0s / 0s],\t\ttrain_loss: 4.9944,\tval_loss: 4.2637\n",
            "14:\t[0s / 0s],\t\ttrain_loss: 4.9890,\tval_loss: 4.2615\n",
            "15:\t[0s / 1s],\t\ttrain_loss: 4.9577,\tval_loss: 4.2591\n",
            "16:\t[0s / 1s],\t\ttrain_loss: 4.9173,\tval_loss: 4.2567\n",
            "17:\t[0s / 1s],\t\ttrain_loss: 4.8878,\tval_loss: 4.2543\n",
            "18:\t[0s / 1s],\t\ttrain_loss: 4.8508,\tval_loss: 4.2518\n",
            "19:\t[0s / 1s],\t\ttrain_loss: 4.8368,\tval_loss: 4.2493\n",
            "20:\t[0s / 1s],\t\ttrain_loss: 4.8230,\tval_loss: 4.2467\n",
            "21:\t[0s / 1s],\t\ttrain_loss: 4.7867,\tval_loss: 4.2439\n",
            "22:\t[0s / 1s],\t\ttrain_loss: 4.7590,\tval_loss: 4.2412\n",
            "23:\t[0s / 1s],\t\ttrain_loss: 4.7367,\tval_loss: 4.2383\n",
            "24:\t[0s / 1s],\t\ttrain_loss: 4.7159,\tval_loss: 4.2355\n",
            "25:\t[0s / 1s],\t\ttrain_loss: 4.6881,\tval_loss: 4.2328\n",
            "26:\t[0s / 1s],\t\ttrain_loss: 4.6658,\tval_loss: 4.2302\n",
            "27:\t[0s / 1s],\t\ttrain_loss: 4.6483,\tval_loss: 4.2279\n",
            "28:\t[0s / 1s],\t\ttrain_loss: 4.6285,\tval_loss: 4.2258\n",
            "29:\t[0s / 1s],\t\ttrain_loss: 4.5969,\tval_loss: 4.2239\n",
            "30:\t[0s / 1s],\t\ttrain_loss: 4.5806,\tval_loss: 4.2223\n",
            "31:\t[0s / 2s],\t\ttrain_loss: 4.5494,\tval_loss: 4.2210\n",
            "32:\t[0s / 2s],\t\ttrain_loss: 4.5333,\tval_loss: 4.2201\n",
            "33:\t[0s / 2s],\t\ttrain_loss: 4.5180,\tval_loss: 4.2199\n",
            "34:\t[0s / 2s],\t\ttrain_loss: 4.4921,\tval_loss: 4.2201\n",
            "35:\t[0s / 2s],\t\ttrain_loss: 4.4779,\tval_loss: 4.2208\n",
            "36:\t[0s / 2s],\t\ttrain_loss: 4.4555,\tval_loss: 4.2219\n",
            "37:\t[0s / 2s],\t\ttrain_loss: 4.4326,\tval_loss: 4.2236\n",
            "38:\t[0s / 2s],\t\ttrain_loss: 4.4243,\tval_loss: 4.2260\n",
            "39:\t[0s / 2s],\t\ttrain_loss: 4.4020,\tval_loss: 4.2291\n",
            "40:\t[0s / 2s],\t\ttrain_loss: 4.3817,\tval_loss: 4.2328\n",
            "41:\t[0s / 2s],\t\ttrain_loss: 4.3660,\tval_loss: 4.2373\n",
            "42:\t[0s / 2s],\t\ttrain_loss: 4.3433,\tval_loss: 4.2426\n",
            "43:\t[0s / 2s],\t\ttrain_loss: 4.3215,\tval_loss: 4.2488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 / 12\n",
            "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
            "Wall time: 9.3 µs\n",
            "0:\t[0s / 0s],\t\ttrain_loss: 5.6313,\tval_loss: 4.2695\n",
            "1:\t[0s / 0s],\t\ttrain_loss: 5.1959,\tval_loss: 4.2622\n",
            "2:\t[0s / 0s],\t\ttrain_loss: 4.9524,\tval_loss: 4.2524\n",
            "3:\t[0s / 0s],\t\ttrain_loss: 4.7523,\tval_loss: 4.2417\n",
            "4:\t[0s / 0s],\t\ttrain_loss: 4.5711,\tval_loss: 4.2300\n",
            "5:\t[0s / 0s],\t\ttrain_loss: 4.4134,\tval_loss: 4.2183\n",
            "6:\t[0s / 0s],\t\ttrain_loss: 4.2835,\tval_loss: 4.2063\n",
            "7:\t[0s / 0s],\t\ttrain_loss: 4.1448,\tval_loss: 4.1940\n",
            "8:\t[0s / 0s],\t\ttrain_loss: 4.0482,\tval_loss: 4.1829\n",
            "9:\t[0s / 0s],\t\ttrain_loss: 3.9455,\tval_loss: 4.1734\n",
            "10:\t[0s / 0s],\t\ttrain_loss: 3.8660,\tval_loss: 4.1650\n",
            "11:\t[0s / 0s],\t\ttrain_loss: 3.7733,\tval_loss: 4.1572\n",
            "12:\t[0s / 0s],\t\ttrain_loss: 3.7444,\tval_loss: 4.1516\n",
            "13:\t[0s / 0s],\t\ttrain_loss: 3.6796,\tval_loss: 4.1437\n",
            "14:\t[0s / 0s],\t\ttrain_loss: 3.8575,\tval_loss: 4.1381\n",
            "15:\t[0s / 0s],\t\ttrain_loss: 3.8222,\tval_loss: 4.1347\n",
            "16:\t[0s / 1s],\t\ttrain_loss: 3.7494,\tval_loss: 4.1324\n",
            "17:\t[0s / 1s],\t\ttrain_loss: 3.8153,\tval_loss: 4.1322\n",
            "18:\t[0s / 1s],\t\ttrain_loss: 3.7214,\tval_loss: 4.1381\n",
            "19:\t[0s / 1s],\t\ttrain_loss: 3.7568,\tval_loss: 4.1456\n",
            "20:\t[0s / 1s],\t\ttrain_loss: 3.7071,\tval_loss: 4.1570\n",
            "21:\t[0s / 1s],\t\ttrain_loss: 3.6629,\tval_loss: 4.1620\n",
            "22:\t[0s / 1s],\t\ttrain_loss: 3.7298,\tval_loss: 4.1692\n",
            "23:\t[0s / 1s],\t\ttrain_loss: 3.8323,\tval_loss: 4.1793\n",
            "24:\t[0s / 1s],\t\ttrain_loss: 3.7530,\tval_loss: 4.1918\n",
            "25:\t[0s / 1s],\t\ttrain_loss: 3.6988,\tval_loss: 4.2040\n",
            "26:\t[0s / 1s],\t\ttrain_loss: 3.6742,\tval_loss: 4.2116\n",
            "27:\t[0s / 1s],\t\ttrain_loss: 3.6752,\tval_loss: 4.2185\n",
            "3 / 12\n",
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 9.06 µs\n",
            "0:\t[0s / 0s],\t\ttrain_loss: 5.6718,\tval_loss: 4.2629\n",
            "1:\t[0s / 0s],\t\ttrain_loss: 5.2813,\tval_loss: 4.2514\n",
            "2:\t[0s / 0s],\t\ttrain_loss: 4.8826,\tval_loss: 4.2334\n",
            "3:\t[0s / 0s],\t\ttrain_loss: 4.6001,\tval_loss: 4.2143\n",
            "4:\t[0s / 0s],\t\ttrain_loss: 4.4877,\tval_loss: 4.1992\n",
            "5:\t[0s / 0s],\t\ttrain_loss: 4.2071,\tval_loss: 4.1847\n",
            "6:\t[0s / 0s],\t\ttrain_loss: 4.0705,\tval_loss: 4.1713\n",
            "7:\t[0s / 0s],\t\ttrain_loss: 3.9940,\tval_loss: 4.1604\n",
            "8:\t[0s / 0s],\t\ttrain_loss: 3.8575,\tval_loss: 4.1502\n",
            "9:\t[0s / 0s],\t\ttrain_loss: 3.7717,\tval_loss: 4.1375\n",
            "10:\t[0s / 0s],\t\ttrain_loss: 4.0110,\tval_loss: 4.1285\n",
            "11:\t[0s / 0s],\t\ttrain_loss: 4.1623,\tval_loss: 4.1251\n",
            "12:\t[0s / 0s],\t\ttrain_loss: 3.7897,\tval_loss: 4.1242\n",
            "13:\t[0s / 0s],\t\ttrain_loss: 3.7483,\tval_loss: 4.1235\n",
            "14:\t[0s / 0s],\t\ttrain_loss: 3.7306,\tval_loss: 4.1222\n",
            "15:\t[0s / 0s],\t\ttrain_loss: 3.7091,\tval_loss: 4.1192\n",
            "16:\t[0s / 1s],\t\ttrain_loss: 3.7093,\tval_loss: 4.1142\n",
            "17:\t[0s / 1s],\t\ttrain_loss: 3.6954,\tval_loss: 4.1069\n",
            "18:\t[0s / 1s],\t\ttrain_loss: 3.7853,\tval_loss: 4.1049\n",
            "19:\t[0s / 1s],\t\ttrain_loss: 3.7315,\tval_loss: 4.1059\n",
            "20:\t[0s / 1s],\t\ttrain_loss: 3.6981,\tval_loss: 4.1096\n",
            "21:\t[0s / 1s],\t\ttrain_loss: 3.6528,\tval_loss: 4.1197\n",
            "22:\t[0s / 1s],\t\ttrain_loss: 3.6473,\tval_loss: 4.1342\n",
            "23:\t[0s / 1s],\t\ttrain_loss: 3.6374,\tval_loss: 4.1524\n",
            "24:\t[0s / 1s],\t\ttrain_loss: 3.6177,\tval_loss: 4.1769\n",
            "25:\t[0s / 1s],\t\ttrain_loss: 3.6082,\tval_loss: 4.2074\n",
            "26:\t[0s / 1s],\t\ttrain_loss: 3.6294,\tval_loss: 4.2439\n",
            "27:\t[0s / 1s],\t\ttrain_loss: 3.6505,\tval_loss: 4.2406\n",
            "28:\t[0s / 1s],\t\ttrain_loss: 4.5096,\tval_loss: 4.2316\n",
            "4 / 12\n",
            "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
            "Wall time: 8.82 µs\n",
            "0:\t[0s / 0s],\t\ttrain_loss: 5.5893,\tval_loss: 4.2668\n",
            "1:\t[0s / 0s],\t\ttrain_loss: 5.5102,\tval_loss: 4.2653\n",
            "2:\t[0s / 0s],\t\ttrain_loss: 5.4363,\tval_loss: 4.2636\n",
            "3:\t[0s / 0s],\t\ttrain_loss: 5.3753,\tval_loss: 4.2618\n",
            "4:\t[0s / 0s],\t\ttrain_loss: 5.2815,\tval_loss: 4.2597\n",
            "5:\t[0s / 0s],\t\ttrain_loss: 5.2325,\tval_loss: 4.2576\n",
            "6:\t[0s / 0s],\t\ttrain_loss: 5.2011,\tval_loss: 4.2552\n",
            "7:\t[0s / 0s],\t\ttrain_loss: 5.1346,\tval_loss: 4.2528\n",
            "8:\t[0s / 0s],\t\ttrain_loss: 5.0899,\tval_loss: 4.2503\n",
            "9:\t[0s / 0s],\t\ttrain_loss: 5.0486,\tval_loss: 4.2476\n",
            "10:\t[0s / 1s],\t\ttrain_loss: 5.0063,\tval_loss: 4.2448\n",
            "11:\t[0s / 1s],\t\ttrain_loss: 4.9769,\tval_loss: 4.2418\n",
            "12:\t[0s / 1s],\t\ttrain_loss: 4.9346,\tval_loss: 4.2385\n",
            "13:\t[0s / 1s],\t\ttrain_loss: 4.8922,\tval_loss: 4.2352\n",
            "14:\t[0s / 1s],\t\ttrain_loss: 4.8640,\tval_loss: 4.2317\n",
            "15:\t[0s / 1s],\t\ttrain_loss: 4.8414,\tval_loss: 4.2281\n",
            "16:\t[0s / 1s],\t\ttrain_loss: 4.8007,\tval_loss: 4.2244\n",
            "17:\t[0s / 1s],\t\ttrain_loss: 4.7689,\tval_loss: 4.2205\n",
            "18:\t[0s / 1s],\t\ttrain_loss: 4.7354,\tval_loss: 4.2165\n",
            "19:\t[0s / 2s],\t\ttrain_loss: 4.7106,\tval_loss: 4.2124\n",
            "20:\t[0s / 2s],\t\ttrain_loss: 4.6841,\tval_loss: 4.2083\n",
            "21:\t[0s / 2s],\t\ttrain_loss: 4.6512,\tval_loss: 4.2041\n",
            "22:\t[0s / 2s],\t\ttrain_loss: 4.6272,\tval_loss: 4.1998\n",
            "23:\t[0s / 2s],\t\ttrain_loss: 4.5952,\tval_loss: 4.1954\n",
            "24:\t[0s / 2s],\t\ttrain_loss: 4.5754,\tval_loss: 4.1912\n",
            "25:\t[0s / 2s],\t\ttrain_loss: 4.5488,\tval_loss: 4.1870\n",
            "26:\t[0s / 2s],\t\ttrain_loss: 4.5213,\tval_loss: 4.1830\n",
            "27:\t[0s / 2s],\t\ttrain_loss: 4.4867,\tval_loss: 4.1793\n",
            "28:\t[0s / 2s],\t\ttrain_loss: 4.4681,\tval_loss: 4.1760\n",
            "29:\t[0s / 2s],\t\ttrain_loss: 4.4472,\tval_loss: 4.1731\n",
            "30:\t[0s / 3s],\t\ttrain_loss: 4.4267,\tval_loss: 4.1707\n",
            "31:\t[0s / 3s],\t\ttrain_loss: 4.4030,\tval_loss: 4.1689\n",
            "32:\t[0s / 3s],\t\ttrain_loss: 4.3783,\tval_loss: 4.1676\n",
            "33:\t[0s / 3s],\t\ttrain_loss: 4.3483,\tval_loss: 4.1670\n",
            "34:\t[0s / 3s],\t\ttrain_loss: 4.3301,\tval_loss: 4.1672\n",
            "35:\t[0s / 3s],\t\ttrain_loss: 4.3042,\tval_loss: 4.1684\n",
            "36:\t[0s / 3s],\t\ttrain_loss: 4.2851,\tval_loss: 4.1704\n",
            "37:\t[0s / 3s],\t\ttrain_loss: 4.2586,\tval_loss: 4.1731\n",
            "38:\t[0s / 3s],\t\ttrain_loss: 4.2493,\tval_loss: 4.1771\n",
            "39:\t[0s / 3s],\t\ttrain_loss: 4.2262,\tval_loss: 4.1819\n",
            "40:\t[0s / 3s],\t\ttrain_loss: 4.1993,\tval_loss: 4.1877\n",
            "41:\t[0s / 3s],\t\ttrain_loss: 4.1884,\tval_loss: 4.1943\n",
            "42:\t[0s / 3s],\t\ttrain_loss: 4.1627,\tval_loss: 4.2022\n",
            "43:\t[0s / 3s],\t\ttrain_loss: 4.1405,\tval_loss: 4.2110\n",
            "5 / 12\n",
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.72 µs\n",
            "0:\t[0s / 0s],\t\ttrain_loss: 5.5466,\tval_loss: 4.2692\n",
            "1:\t[0s / 0s],\t\ttrain_loss: 5.1121,\tval_loss: 4.2595\n",
            "2:\t[0s / 0s],\t\ttrain_loss: 4.8254,\tval_loss: 4.2482\n",
            "3:\t[0s / 0s],\t\ttrain_loss: 4.6008,\tval_loss: 4.2357\n",
            "4:\t[0s / 0s],\t\ttrain_loss: 4.3932,\tval_loss: 4.2213\n",
            "5:\t[0s / 0s],\t\ttrain_loss: 4.2366,\tval_loss: 4.2057\n",
            "6:\t[0s / 0s],\t\ttrain_loss: 4.0886,\tval_loss: 4.1906\n",
            "7:\t[0s / 0s],\t\ttrain_loss: 3.9688,\tval_loss: 4.1760\n",
            "8:\t[0s / 0s],\t\ttrain_loss: 3.9110,\tval_loss: 4.1644\n",
            "9:\t[0s / 0s],\t\ttrain_loss: 3.8774,\tval_loss: 4.1551\n",
            "10:\t[0s / 0s],\t\ttrain_loss: 3.7541,\tval_loss: 4.1465\n",
            "11:\t[0s / 1s],\t\ttrain_loss: 3.6919,\tval_loss: 4.1378\n",
            "12:\t[0s / 1s],\t\ttrain_loss: 3.6509,\tval_loss: 4.1275\n",
            "13:\t[0s / 1s],\t\ttrain_loss: 3.9137,\tval_loss: 4.1219\n",
            "14:\t[0s / 1s],\t\ttrain_loss: 3.7406,\tval_loss: 4.1196\n",
            "15:\t[0s / 1s],\t\ttrain_loss: 3.7483,\tval_loss: 4.1230\n",
            "16:\t[0s / 1s],\t\ttrain_loss: 3.8122,\tval_loss: 4.1273\n",
            "17:\t[0s / 1s],\t\ttrain_loss: 3.7137,\tval_loss: 4.1385\n",
            "18:\t[0s / 1s],\t\ttrain_loss: 3.6773,\tval_loss: 4.1450\n",
            "19:\t[0s / 1s],\t\ttrain_loss: 3.7806,\tval_loss: 4.1536\n",
            "20:\t[0s / 1s],\t\ttrain_loss: 3.6842,\tval_loss: 4.1655\n",
            "21:\t[0s / 1s],\t\ttrain_loss: 3.6801,\tval_loss: 4.1732\n",
            "22:\t[0s / 1s],\t\ttrain_loss: 3.8238,\tval_loss: 4.1830\n",
            "23:\t[0s / 1s],\t\ttrain_loss: 3.7216,\tval_loss: 4.1934\n",
            "24:\t[0s / 1s],\t\ttrain_loss: 3.6856,\tval_loss: 4.2074\n",
            "6 / 12\n",
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 6.2 µs\n",
            "0:\t[0s / 0s],\t\ttrain_loss: 5.6177,\tval_loss: 4.2749\n",
            "1:\t[0s / 0s],\t\ttrain_loss: 5.1740,\tval_loss: 4.2662\n",
            "2:\t[0s / 0s],\t\ttrain_loss: 4.9136,\tval_loss: 4.2563\n",
            "3:\t[0s / 0s],\t\ttrain_loss: 4.6956,\tval_loss: 4.2452\n",
            "4:\t[0s / 0s],\t\ttrain_loss: 4.5046,\tval_loss: 4.2330\n",
            "5:\t[0s / 0s],\t\ttrain_loss: 4.3417,\tval_loss: 4.2199\n",
            "6:\t[0s / 0s],\t\ttrain_loss: 4.2047,\tval_loss: 4.2074\n",
            "7:\t[0s / 0s],\t\ttrain_loss: 4.0686,\tval_loss: 4.1948\n",
            "8:\t[0s / 0s],\t\ttrain_loss: 3.9524,\tval_loss: 4.1822\n",
            "9:\t[0s / 0s],\t\ttrain_loss: 3.9117,\tval_loss: 4.1712\n",
            "10:\t[0s / 0s],\t\ttrain_loss: 3.8227,\tval_loss: 4.1633\n",
            "11:\t[0s / 0s],\t\ttrain_loss: 3.7254,\tval_loss: 4.1558\n",
            "12:\t[0s / 0s],\t\ttrain_loss: 3.6942,\tval_loss: 4.1480\n",
            "13:\t[0s / 0s],\t\ttrain_loss: 3.7014,\tval_loss: 4.1373\n",
            "14:\t[0s / 0s],\t\ttrain_loss: 4.1813,\tval_loss: 4.1312\n",
            "15:\t[0s / 0s],\t\ttrain_loss: 4.3775,\tval_loss: 4.1291\n",
            "16:\t[0s / 0s],\t\ttrain_loss: 4.2000,\tval_loss: 4.1306\n",
            "17:\t[0s / 0s],\t\ttrain_loss: 3.9508,\tval_loss: 4.1348\n",
            "18:\t[0s / 0s],\t\ttrain_loss: 3.8093,\tval_loss: 4.1416\n",
            "19:\t[0s / 0s],\t\ttrain_loss: 3.7412,\tval_loss: 4.1435\n",
            "20:\t[0s / 0s],\t\ttrain_loss: 3.8338,\tval_loss: 4.1470\n",
            "21:\t[0s / 0s],\t\ttrain_loss: 3.8055,\tval_loss: 4.1532\n",
            "22:\t[0s / 0s],\t\ttrain_loss: 3.7657,\tval_loss: 4.1598\n",
            "23:\t[0s / 0s],\t\ttrain_loss: 3.7375,\tval_loss: 4.1665\n",
            "24:\t[0s / 0s],\t\ttrain_loss: 3.7288,\tval_loss: 4.1713\n",
            "25:\t[0s / 0s],\t\ttrain_loss: 3.7184,\tval_loss: 4.1782\n",
            "7 / 12\n",
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 9.54 µs\n",
            "0:\t[0s / 0s],\t\ttrain_loss: 5.7092,\tval_loss: 4.2741\n",
            "1:\t[0s / 0s],\t\ttrain_loss: 5.3907,\tval_loss: 4.2706\n",
            "2:\t[0s / 1s],\t\ttrain_loss: 5.1786,\tval_loss: 4.2668\n",
            "3:\t[0s / 1s],\t\ttrain_loss: 5.0208,\tval_loss: 4.2627\n",
            "4:\t[0s / 1s],\t\ttrain_loss: 4.8976,\tval_loss: 4.2582\n",
            "5:\t[0s / 2s],\t\ttrain_loss: 4.7717,\tval_loss: 4.2536\n",
            "6:\t[0s / 2s],\t\ttrain_loss: 4.6672,\tval_loss: 4.2486\n",
            "7:\t[0s / 2s],\t\ttrain_loss: 4.5665,\tval_loss: 4.2430\n",
            "8:\t[0s / 2s],\t\ttrain_loss: 4.4691,\tval_loss: 4.2368\n",
            "9:\t[0s / 3s],\t\ttrain_loss: 4.3872,\tval_loss: 4.2297\n",
            "10:\t[0s / 3s],\t\ttrain_loss: 4.3060,\tval_loss: 4.2216\n",
            "11:\t[0s / 3s],\t\ttrain_loss: 4.2279,\tval_loss: 4.2128\n",
            "12:\t[0s / 3s],\t\ttrain_loss: 4.1774,\tval_loss: 4.2030\n",
            "13:\t[0s / 3s],\t\ttrain_loss: 4.1250,\tval_loss: 4.1923\n",
            "14:\t[0s / 3s],\t\ttrain_loss: 4.0430,\tval_loss: 4.1805\n",
            "15:\t[0s / 3s],\t\ttrain_loss: 4.0193,\tval_loss: 4.1686\n",
            "16:\t[0s / 4s],\t\ttrain_loss: 3.9725,\tval_loss: 4.1567\n",
            "17:\t[0s / 4s],\t\ttrain_loss: 3.9260,\tval_loss: 4.1446\n",
            "18:\t[0s / 4s],\t\ttrain_loss: 3.8923,\tval_loss: 4.1327\n",
            "19:\t[0s / 5s],\t\ttrain_loss: 3.8631,\tval_loss: 4.1217\n",
            "20:\t[0s / 5s],\t\ttrain_loss: 3.8012,\tval_loss: 4.1110\n",
            "21:\t[0s / 6s],\t\ttrain_loss: 3.7822,\tval_loss: 4.1006\n",
            "22:\t[0s / 6s],\t\ttrain_loss: 3.8034,\tval_loss: 4.0910\n",
            "23:\t[0s / 7s],\t\ttrain_loss: 3.7722,\tval_loss: 4.0850\n",
            "24:\t[0s / 7s],\t\ttrain_loss: 3.7548,\tval_loss: 4.0819\n",
            "25:\t[0s / 8s],\t\ttrain_loss: 3.7311,\tval_loss: 4.0805\n",
            "26:\t[0s / 8s],\t\ttrain_loss: 3.7592,\tval_loss: 4.0809\n",
            "27:\t[0s / 8s],\t\ttrain_loss: 3.7384,\tval_loss: 4.0829\n",
            "28:\t[0s / 8s],\t\ttrain_loss: 3.6904,\tval_loss: 4.0846\n",
            "29:\t[0s / 8s],\t\ttrain_loss: 3.7212,\tval_loss: 4.0873\n",
            "30:\t[0s / 8s],\t\ttrain_loss: 3.7425,\tval_loss: 4.0909\n",
            "31:\t[0s / 8s],\t\ttrain_loss: 3.7352,\tval_loss: 4.0941\n",
            "32:\t[0s / 8s],\t\ttrain_loss: 3.7772,\tval_loss: 4.0997\n",
            "33:\t[0s / 8s],\t\ttrain_loss: 3.7002,\tval_loss: 4.1042\n",
            "34:\t[0s / 8s],\t\ttrain_loss: 3.6874,\tval_loss: 4.1142\n",
            "35:\t[0s / 8s],\t\ttrain_loss: 3.7441,\tval_loss: 4.1276\n",
            "8 / 12\n",
            "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
            "Wall time: 5.96 µs\n",
            "0:\t[0s / 0s],\t\ttrain_loss: 5.6751,\tval_loss: 4.2595\n",
            "1:\t[0s / 0s],\t\ttrain_loss: 6.3188,\tval_loss: 4.2479\n",
            "2:\t[0s / 0s],\t\ttrain_loss: 5.9157,\tval_loss: 4.2414\n",
            "3:\t[0s / 0s],\t\ttrain_loss: 5.4882,\tval_loss: 4.2342\n",
            "4:\t[0s / 0s],\t\ttrain_loss: 5.1043,\tval_loss: 4.2251\n",
            "5:\t[0s / 0s],\t\ttrain_loss: 4.9850,\tval_loss: 4.2128\n",
            "6:\t[0s / 0s],\t\ttrain_loss: 4.8269,\tval_loss: 4.1998\n",
            "7:\t[0s / 0s],\t\ttrain_loss: 4.6454,\tval_loss: 4.1895\n",
            "8:\t[0s / 0s],\t\ttrain_loss: 4.5709,\tval_loss: 4.1808\n",
            "9:\t[0s / 1s],\t\ttrain_loss: 4.4679,\tval_loss: 4.1738\n",
            "10:\t[0s / 1s],\t\ttrain_loss: 4.4127,\tval_loss: 4.1709\n",
            "11:\t[0s / 2s],\t\ttrain_loss: 4.2600,\tval_loss: 4.1683\n",
            "12:\t[0s / 2s],\t\ttrain_loss: 4.1820,\tval_loss: 4.1650\n",
            "13:\t[0s / 2s],\t\ttrain_loss: 4.1098,\tval_loss: 4.1594\n",
            "14:\t[0s / 3s],\t\ttrain_loss: 4.0744,\tval_loss: 4.1508\n",
            "15:\t[0s / 3s],\t\ttrain_loss: 4.0253,\tval_loss: 4.1398\n",
            "16:\t[0s / 4s],\t\ttrain_loss: 3.9744,\tval_loss: 4.1264\n",
            "17:\t[0s / 4s],\t\ttrain_loss: 3.9535,\tval_loss: 4.1115\n",
            "18:\t[0s / 5s],\t\ttrain_loss: 3.9073,\tval_loss: 4.0958\n",
            "19:\t[0s / 5s],\t\ttrain_loss: 3.8513,\tval_loss: 4.0799\n",
            "20:\t[0s / 5s],\t\ttrain_loss: 3.7785,\tval_loss: 4.0648\n",
            "21:\t[0s / 5s],\t\ttrain_loss: 3.7460,\tval_loss: 4.0540\n",
            "22:\t[0s / 5s],\t\ttrain_loss: 3.7339,\tval_loss: 4.0490\n",
            "23:\t[0s / 5s],\t\ttrain_loss: 3.7541,\tval_loss: 4.0516\n",
            "24:\t[0s / 5s],\t\ttrain_loss: 3.7337,\tval_loss: 4.0604\n",
            "25:\t[0s / 5s],\t\ttrain_loss: 3.9023,\tval_loss: 4.0646\n",
            "26:\t[0s / 5s],\t\ttrain_loss: 3.7434,\tval_loss: 4.0901\n",
            "27:\t[0s / 5s],\t\ttrain_loss: 3.8529,\tval_loss: 4.1497\n",
            "28:\t[0s / 5s],\t\ttrain_loss: 4.6000,\tval_loss: 4.1933\n",
            "29:\t[0s / 5s],\t\ttrain_loss: 4.3597,\tval_loss: 4.2040\n",
            "30:\t[0s / 5s],\t\ttrain_loss: 3.9058,\tval_loss: 4.2155\n",
            "31:\t[0s / 5s],\t\ttrain_loss: 3.9419,\tval_loss: 4.2241\n",
            "32:\t[0s / 5s],\t\ttrain_loss: 3.9720,\tval_loss: 4.2271\n",
            "9 / 12\n",
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.96 µs\n",
            "0:\t[0s / 0s],\t\ttrain_loss: 5.5549,\tval_loss: 4.2735\n",
            "1:\t[0s / 0s],\t\ttrain_loss: 5.2836,\tval_loss: 4.2701\n",
            "2:\t[0s / 1s],\t\ttrain_loss: 5.0863,\tval_loss: 4.2663\n",
            "3:\t[0s / 1s],\t\ttrain_loss: 4.9489,\tval_loss: 4.2620\n",
            "4:\t[0s / 1s],\t\ttrain_loss: 4.8072,\tval_loss: 4.2573\n",
            "5:\t[0s / 2s],\t\ttrain_loss: 4.6870,\tval_loss: 4.2523\n",
            "6:\t[0s / 2s],\t\ttrain_loss: 4.5585,\tval_loss: 4.2465\n",
            "7:\t[0s / 3s],\t\ttrain_loss: 4.4720,\tval_loss: 4.2399\n",
            "8:\t[0s / 3s],\t\ttrain_loss: 4.3843,\tval_loss: 4.2324\n",
            "9:\t[0s / 3s],\t\ttrain_loss: 4.3000,\tval_loss: 4.2240\n",
            "10:\t[0s / 3s],\t\ttrain_loss: 4.2398,\tval_loss: 4.2148\n",
            "11:\t[0s / 3s],\t\ttrain_loss: 4.1551,\tval_loss: 4.2046\n",
            "12:\t[0s / 3s],\t\ttrain_loss: 4.0821,\tval_loss: 4.1933\n",
            "13:\t[0s / 3s],\t\ttrain_loss: 4.0587,\tval_loss: 4.1811\n",
            "14:\t[0s / 3s],\t\ttrain_loss: 3.9818,\tval_loss: 4.1684\n",
            "15:\t[0s / 3s],\t\ttrain_loss: 3.9278,\tval_loss: 4.1553\n",
            "16:\t[0s / 3s],\t\ttrain_loss: 3.9132,\tval_loss: 4.1418\n",
            "17:\t[0s / 4s],\t\ttrain_loss: 3.8875,\tval_loss: 4.1279\n",
            "18:\t[0s / 4s],\t\ttrain_loss: 3.8595,\tval_loss: 4.1142\n",
            "19:\t[0s / 4s],\t\ttrain_loss: 3.8487,\tval_loss: 4.1012\n",
            "20:\t[0s / 5s],\t\ttrain_loss: 3.7915,\tval_loss: 4.0906\n",
            "21:\t[0s / 5s],\t\ttrain_loss: 3.7560,\tval_loss: 4.0820\n",
            "22:\t[0s / 5s],\t\ttrain_loss: 3.7800,\tval_loss: 4.0742\n",
            "23:\t[0s / 5s],\t\ttrain_loss: 3.7706,\tval_loss: 4.0669\n",
            "24:\t[0s / 6s],\t\ttrain_loss: 3.7398,\tval_loss: 4.0619\n",
            "25:\t[0s / 6s],\t\ttrain_loss: 3.7402,\tval_loss: 4.0585\n",
            "26:\t[0s / 7s],\t\ttrain_loss: 3.7448,\tval_loss: 4.0571\n",
            "27:\t[0s / 7s],\t\ttrain_loss: 3.7279,\tval_loss: 4.0598\n",
            "28:\t[0s / 7s],\t\ttrain_loss: 3.7373,\tval_loss: 4.0631\n",
            "29:\t[0s / 7s],\t\ttrain_loss: 3.8302,\tval_loss: 4.0681\n",
            "30:\t[0s / 7s],\t\ttrain_loss: 3.8608,\tval_loss: 4.0744\n",
            "31:\t[0s / 7s],\t\ttrain_loss: 3.7250,\tval_loss: 4.0875\n",
            "32:\t[0s / 7s],\t\ttrain_loss: 3.7476,\tval_loss: 4.1003\n",
            "33:\t[0s / 7s],\t\ttrain_loss: 3.7772,\tval_loss: 4.1136\n",
            "34:\t[0s / 7s],\t\ttrain_loss: 4.0051,\tval_loss: 4.1275\n",
            "35:\t[0s / 7s],\t\ttrain_loss: 4.1579,\tval_loss: 4.1410\n",
            "36:\t[0s / 7s],\t\ttrain_loss: 3.8496,\tval_loss: 4.1540\n",
            "10 / 12\n",
            "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
            "Wall time: 6.2 µs\n",
            "0:\t[0s / 0s],\t\ttrain_loss: 5.6327,\tval_loss: 4.2692\n",
            "1:\t[1s / 1s],\t\ttrain_loss: 5.1963,\tval_loss: 4.2637\n",
            "2:\t[0s / 1s],\t\ttrain_loss: 4.9652,\tval_loss: 4.2583\n",
            "3:\t[0s / 1s],\t\ttrain_loss: 4.7452,\tval_loss: 4.2526\n",
            "4:\t[0s / 1s],\t\ttrain_loss: 4.5882,\tval_loss: 4.2458\n",
            "5:\t[0s / 1s],\t\ttrain_loss: 4.4262,\tval_loss: 4.2379\n",
            "6:\t[0s / 1s],\t\ttrain_loss: 4.3130,\tval_loss: 4.2287\n",
            "7:\t[0s / 2s],\t\ttrain_loss: 4.1809,\tval_loss: 4.2186\n",
            "8:\t[0s / 2s],\t\ttrain_loss: 4.0714,\tval_loss: 4.2078\n",
            "9:\t[0s / 2s],\t\ttrain_loss: 3.9891,\tval_loss: 4.1965\n",
            "10:\t[0s / 2s],\t\ttrain_loss: 3.9446,\tval_loss: 4.1845\n",
            "11:\t[0s / 2s],\t\ttrain_loss: 3.8580,\tval_loss: 4.1718\n",
            "12:\t[0s / 2s],\t\ttrain_loss: 3.8552,\tval_loss: 4.1589\n",
            "13:\t[0s / 2s],\t\ttrain_loss: 3.8386,\tval_loss: 4.1464\n",
            "14:\t[0s / 3s],\t\ttrain_loss: 3.7613,\tval_loss: 4.1339\n",
            "15:\t[0s / 4s],\t\ttrain_loss: 3.7703,\tval_loss: 4.1218\n",
            "16:\t[0s / 5s],\t\ttrain_loss: 3.7850,\tval_loss: 4.1121\n",
            "17:\t[0s / 6s],\t\ttrain_loss: 3.7991,\tval_loss: 4.1015\n",
            "18:\t[0s / 6s],\t\ttrain_loss: 3.8068,\tval_loss: 4.0937\n",
            "19:\t[0s / 6s],\t\ttrain_loss: 3.9970,\tval_loss: 4.0873\n",
            "20:\t[0s / 6s],\t\ttrain_loss: 3.8278,\tval_loss: 4.0813\n",
            "21:\t[0s / 7s],\t\ttrain_loss: 3.7310,\tval_loss: 4.0738\n",
            "22:\t[0s / 7s],\t\ttrain_loss: 3.7664,\tval_loss: 4.0662\n",
            "23:\t[0s / 7s],\t\ttrain_loss: 3.9830,\tval_loss: 4.0604\n",
            "24:\t[0s / 7s],\t\ttrain_loss: 3.9400,\tval_loss: 4.0576\n",
            "25:\t[0s / 7s],\t\ttrain_loss: 3.7413,\tval_loss: 4.0553\n",
            "26:\t[0s / 7s],\t\ttrain_loss: 3.8320,\tval_loss: 4.0553\n",
            "27:\t[0s / 7s],\t\ttrain_loss: 3.7512,\tval_loss: 4.0579\n",
            "28:\t[0s / 8s],\t\ttrain_loss: 3.7458,\tval_loss: 4.0622\n",
            "29:\t[0s / 8s],\t\ttrain_loss: 3.7546,\tval_loss: 4.0691\n",
            "30:\t[0s / 8s],\t\ttrain_loss: 3.7904,\tval_loss: 4.0779\n",
            "31:\t[0s / 8s],\t\ttrain_loss: 3.7262,\tval_loss: 4.0879\n",
            "32:\t[0s / 8s],\t\ttrain_loss: 3.7187,\tval_loss: 4.0985\n",
            "33:\t[0s / 8s],\t\ttrain_loss: 3.7636,\tval_loss: 4.1095\n",
            "34:\t[0s / 8s],\t\ttrain_loss: 3.7797,\tval_loss: 4.1218\n",
            "35:\t[0s / 8s],\t\ttrain_loss: 3.7347,\tval_loss: 4.1356\n",
            "36:\t[0s / 8s],\t\ttrain_loss: 3.6830,\tval_loss: 4.1529\n",
            "11 / 12\n",
            "CPU times: user 17 µs, sys: 0 ns, total: 17 µs\n",
            "Wall time: 19.8 µs\n",
            "0:\t[0s / 0s],\t\ttrain_loss: 5.6123,\tval_loss: 4.2279\n",
            "1:\t[4s / 4s],\t\ttrain_loss: 8.1140,\tval_loss: 4.2273\n",
            "2:\t[0s / 4s],\t\ttrain_loss: 8.2837,\tval_loss: 4.2431\n",
            "3:\t[0s / 4s],\t\ttrain_loss: 7.5552,\tval_loss: 4.2500\n",
            "4:\t[0s / 4s],\t\ttrain_loss: 7.0814,\tval_loss: 4.2527\n",
            "5:\t[0s / 4s],\t\ttrain_loss: 6.3423,\tval_loss: 4.2526\n",
            "6:\t[0s / 4s],\t\ttrain_loss: 5.9429,\tval_loss: 4.2477\n",
            "7:\t[0s / 4s],\t\ttrain_loss: 5.6325,\tval_loss: 4.2404\n",
            "8:\t[0s / 4s],\t\ttrain_loss: 5.3227,\tval_loss: 4.2329\n",
            "9:\t[0s / 5s],\t\ttrain_loss: 5.1299,\tval_loss: 4.2262\n",
            "10:\t[0s / 6s],\t\ttrain_loss: 4.9914,\tval_loss: 4.2200\n",
            "11:\t[0s / 7s],\t\ttrain_loss: 4.8265,\tval_loss: 4.2140\n",
            "12:\t[0s / 8s],\t\ttrain_loss: 4.7444,\tval_loss: 4.2086\n",
            "13:\t[0s / 8s],\t\ttrain_loss: 4.6826,\tval_loss: 4.2030\n",
            "14:\t[0s / 8s],\t\ttrain_loss: 4.6175,\tval_loss: 4.1966\n",
            "15:\t[0s / 8s],\t\ttrain_loss: 4.5540,\tval_loss: 4.1876\n",
            "16:\t[0s / 9s],\t\ttrain_loss: 4.4840,\tval_loss: 4.1748\n",
            "17:\t[0s / 9s],\t\ttrain_loss: 4.4355,\tval_loss: 4.1580\n",
            "18:\t[0s / 9s],\t\ttrain_loss: 4.3345,\tval_loss: 4.1387\n",
            "19:\t[0s / 9s],\t\ttrain_loss: 4.2461,\tval_loss: 4.1188\n",
            "20:\t[0s / 10s],\t\ttrain_loss: 4.1520,\tval_loss: 4.0990\n",
            "21:\t[0s / 11s],\t\ttrain_loss: 4.0561,\tval_loss: 4.0797\n",
            "22:\t[5s / 16s],\t\ttrain_loss: 3.9806,\tval_loss: 4.0638\n",
            "23:\t[1s / 18s],\t\ttrain_loss: 3.9471,\tval_loss: 4.0526\n",
            "24:\t[1s / 19s],\t\ttrain_loss: 3.9333,\tval_loss: 4.0456\n",
            "25:\t[1s / 20s],\t\ttrain_loss: 3.8150,\tval_loss: 4.0444\n",
            "26:\t[0s / 20s],\t\ttrain_loss: 3.7779,\tval_loss: 4.0495\n",
            "27:\t[0s / 20s],\t\ttrain_loss: 3.7311,\tval_loss: 4.0593\n",
            "28:\t[0s / 20s],\t\ttrain_loss: 3.7328,\tval_loss: 4.0736\n",
            "29:\t[0s / 20s],\t\ttrain_loss: 3.7268,\tval_loss: 4.0883\n",
            "30:\t[0s / 20s],\t\ttrain_loss: 4.3300,\tval_loss: 4.1073\n",
            "31:\t[0s / 20s],\t\ttrain_loss: 3.7483,\tval_loss: 4.1293\n",
            "32:\t[0s / 20s],\t\ttrain_loss: 3.7781,\tval_loss: 4.1469\n",
            "33:\t[0s / 21s],\t\ttrain_loss: 3.8078,\tval_loss: 4.1650\n",
            "34:\t[0s / 21s],\t\ttrain_loss: 3.7632,\tval_loss: 4.1863\n",
            "35:\t[0s / 21s],\t\ttrain_loss: 3.7587,\tval_loss: 4.2113\n",
            "12 / 12\n",
            "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
            "Wall time: 9.54 µs\n",
            "0:\t[0s / 0s],\t\ttrain_loss: 5.6010,\tval_loss: 4.2734\n",
            "1:\t[1s / 1s],\t\ttrain_loss: 5.1565,\tval_loss: 4.2680\n",
            "2:\t[1s / 2s],\t\ttrain_loss: 4.9069,\tval_loss: 4.2624\n",
            "3:\t[1s / 3s],\t\ttrain_loss: 4.7196,\tval_loss: 4.2564\n",
            "4:\t[1s / 5s],\t\ttrain_loss: 4.5377,\tval_loss: 4.2496\n",
            "5:\t[0s / 5s],\t\ttrain_loss: 4.3906,\tval_loss: 4.2420\n",
            "6:\t[0s / 5s],\t\ttrain_loss: 4.2703,\tval_loss: 4.2337\n",
            "7:\t[0s / 6s],\t\ttrain_loss: 4.1369,\tval_loss: 4.2249\n",
            "8:\t[1s / 7s],\t\ttrain_loss: 4.0619,\tval_loss: 4.2153\n",
            "9:\t[0s / 7s],\t\ttrain_loss: 4.0060,\tval_loss: 4.2045\n",
            "10:\t[0s / 8s],\t\ttrain_loss: 3.9840,\tval_loss: 4.1934\n",
            "11:\t[1s / 9s],\t\ttrain_loss: 3.8804,\tval_loss: 4.1825\n",
            "12:\t[0s / 10s],\t\ttrain_loss: 3.8062,\tval_loss: 4.1710\n",
            "13:\t[0s / 10s],\t\ttrain_loss: 3.7900,\tval_loss: 4.1591\n",
            "14:\t[0s / 10s],\t\ttrain_loss: 3.7465,\tval_loss: 4.1480\n",
            "15:\t[0s / 10s],\t\ttrain_loss: 3.8098,\tval_loss: 4.1385\n",
            "16:\t[0s / 10s],\t\ttrain_loss: 3.7289,\tval_loss: 4.1295\n",
            "17:\t[0s / 10s],\t\ttrain_loss: 3.9070,\tval_loss: 4.1205\n",
            "18:\t[0s / 11s],\t\ttrain_loss: 3.9027,\tval_loss: 4.1113\n",
            "19:\t[0s / 11s],\t\ttrain_loss: 3.7621,\tval_loss: 4.1020\n",
            "20:\t[0s / 11s],\t\ttrain_loss: 3.7667,\tval_loss: 4.0942\n",
            "21:\t[0s / 11s],\t\ttrain_loss: 3.7903,\tval_loss: 4.0869\n",
            "22:\t[0s / 11s],\t\ttrain_loss: 3.8355,\tval_loss: 4.0814\n",
            "23:\t[0s / 12s],\t\ttrain_loss: 3.7195,\tval_loss: 4.0787\n",
            "24:\t[5s / 17s],\t\ttrain_loss: 3.7176,\tval_loss: 4.0758\n",
            "25:\t[0s / 17s],\t\ttrain_loss: 3.9591,\tval_loss: 4.0750\n",
            "26:\t[0s / 17s],\t\ttrain_loss: 3.7566,\tval_loss: 4.0760\n",
            "27:\t[0s / 18s],\t\ttrain_loss: 3.6843,\tval_loss: 4.0764\n",
            "28:\t[0s / 18s],\t\ttrain_loss: 3.7671,\tval_loss: 4.0769\n",
            "29:\t[0s / 18s],\t\ttrain_loss: 4.0118,\tval_loss: 4.0786\n",
            "30:\t[0s / 18s],\t\ttrain_loss: 4.2418,\tval_loss: 4.0825\n",
            "31:\t[0s / 18s],\t\ttrain_loss: 3.8015,\tval_loss: 4.0887\n",
            "32:\t[0s / 18s],\t\ttrain_loss: 3.7246,\tval_loss: 4.0957\n",
            "33:\t[0s / 18s],\t\ttrain_loss: 3.7514,\tval_loss: 4.1062\n",
            "34:\t[0s / 18s],\t\ttrain_loss: 4.0450,\tval_loss: 4.1181\n",
            "35:\t[0s / 18s],\t\ttrain_loss: 3.9635,\tval_loss: 4.1318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keho7amdxDPt"
      },
      "source": [
        "- brier_score가 가장 작은 것부터 정렬\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-vyUqxRtULi"
      },
      "source": [
        "brier_scores.sort()\n",
        "selected_genes = []\n",
        "for i in range(10):\n",
        "  selected_genes.append(brier_scores[0][4][i][1])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q65Qjo4A3UPF",
        "outputId": "a660bcac-22f8-4a90-aba3-ca5a586719d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gene_count= {}\n",
        "for i in brier_scores:\n",
        "  lst = []\n",
        "  for j in i[4]:\n",
        "    lst.append(j[1])\n",
        "  print(lst)\n",
        "  for k in lst:\n",
        "    if k in gene_count.keys():\n",
        "      gene_count[k] += 1\n",
        "    else:\n",
        "      gene_count[k] = 1\n",
        "sorted(gene_count.items(), key=lambda x: x[1], reverse=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['G221', 'G255', 'G285', 'G243', 'G120', 'G215', 'G35', 'G67', 'G140', 'G125']\n",
            "['G192', 'G284', 'G3', 'G243', 'G259', 'G217', 'G254', 'G127', 'G32', 'G182']\n",
            "['G206', 'G14', 'G136', 'G21', 'G214', 'G86', 'G139', 'G164', 'G29', 'G203']\n",
            "['G150', 'G60', 'G157', 'G43', 'G101', 'G14', 'G275', 'G137', 'G112', 'G243']\n",
            "['G147', 'G236', 'G215', 'G174', 'G242', 'G119', 'G265', 'G149', 'G259', 'G180']\n",
            "['G35', 'G118', 'G84', 'G186', 'G217', 'G193', 'G11', 'G258', 'G26', 'G34']\n",
            "['G35', 'G274', 'G72', 'G149', 'G284', 'G34', 'G60', 'G77', 'G248', 'G43']\n",
            "['G69', 'G153', 'G120', 'G284', 'G193', 'G205', 'G58', 'G136', 'G180', 'G37']\n",
            "['G194', 'G165', 'G89', 'G185', 'G221', 'G183', 'G265', 'G120', 'G230', 'G60']\n",
            "['G139', 'G236', 'G195', 'G252', 'G277', 'G242', 'G174', 'G230', 'G111', 'G35']\n",
            "['G234', 'G175', 'G279', 'G127', 'G75', 'G55', 'G241', 'G262', 'G135', 'G275']\n",
            "['G183', 'G256', 'G62', 'G75', 'G175', 'G220', 'G182', 'G118', 'G193', 'G35']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('G35', 5),\n",
              " ('G243', 3),\n",
              " ('G120', 3),\n",
              " ('G284', 3),\n",
              " ('G60', 3),\n",
              " ('G193', 3),\n",
              " ('G221', 2),\n",
              " ('G215', 2),\n",
              " ('G259', 2),\n",
              " ('G217', 2),\n",
              " ('G127', 2),\n",
              " ('G182', 2),\n",
              " ('G14', 2),\n",
              " ('G136', 2),\n",
              " ('G139', 2),\n",
              " ('G43', 2),\n",
              " ('G275', 2),\n",
              " ('G236', 2),\n",
              " ('G174', 2),\n",
              " ('G242', 2),\n",
              " ('G265', 2),\n",
              " ('G149', 2),\n",
              " ('G180', 2),\n",
              " ('G118', 2),\n",
              " ('G34', 2),\n",
              " ('G183', 2),\n",
              " ('G230', 2),\n",
              " ('G175', 2),\n",
              " ('G75', 2),\n",
              " ('G255', 1),\n",
              " ('G285', 1),\n",
              " ('G67', 1),\n",
              " ('G140', 1),\n",
              " ('G125', 1),\n",
              " ('G192', 1),\n",
              " ('G3', 1),\n",
              " ('G254', 1),\n",
              " ('G32', 1),\n",
              " ('G206', 1),\n",
              " ('G21', 1),\n",
              " ('G214', 1),\n",
              " ('G86', 1),\n",
              " ('G164', 1),\n",
              " ('G29', 1),\n",
              " ('G203', 1),\n",
              " ('G150', 1),\n",
              " ('G157', 1),\n",
              " ('G101', 1),\n",
              " ('G137', 1),\n",
              " ('G112', 1),\n",
              " ('G147', 1),\n",
              " ('G119', 1),\n",
              " ('G84', 1),\n",
              " ('G186', 1),\n",
              " ('G11', 1),\n",
              " ('G258', 1),\n",
              " ('G26', 1),\n",
              " ('G274', 1),\n",
              " ('G72', 1),\n",
              " ('G77', 1),\n",
              " ('G248', 1),\n",
              " ('G69', 1),\n",
              " ('G153', 1),\n",
              " ('G205', 1),\n",
              " ('G58', 1),\n",
              " ('G37', 1),\n",
              " ('G194', 1),\n",
              " ('G165', 1),\n",
              " ('G89', 1),\n",
              " ('G185', 1),\n",
              " ('G195', 1),\n",
              " ('G252', 1),\n",
              " ('G277', 1),\n",
              " ('G111', 1),\n",
              " ('G234', 1),\n",
              " ('G279', 1),\n",
              " ('G55', 1),\n",
              " ('G241', 1),\n",
              " ('G262', 1),\n",
              " ('G135', 1),\n",
              " ('G256', 1),\n",
              " ('G62', 1),\n",
              " ('G220', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yO9ti8SxLPm"
      },
      "source": [
        "- brier_score가 가장 좋은 케이스 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd2yOAOz5jME",
        "outputId": "8137416d-70d0-4e66-e902-41e531d90f38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Brier Score :\", brier_scores[0][0])\n",
        "print(\"Hidden Layers :\", brier_scores[0][1])\n",
        "print(\"Number of Nodes :\", brier_scores[0][2])\n",
        "print(\"Learning Rate :\", brier_scores[0][3])\n",
        "print(\"Selection :\", selected_genes)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brier Score : 0.0979749663307075\n",
            "Hidden Layers : 2\n",
            "Number of Nodes : 2000\n",
            "Learning Rate : 0.001\n",
            "Selection : ['G221', 'G255', 'G285', 'G243', 'G120', 'G215', 'G35', 'G67', 'G140', 'G125']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-mmybRE9XRp"
      },
      "source": [
        "NAN 값을 기준으로 정렬이 끊어진 문제 발견"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPQucTT270bt",
        "outputId": "ccc4a5ca-0450-42ba-d7fb-ea36f3b659ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"brier_score, hidden layer, number of nodes, learning rate\")\n",
        "for i in brier_scores:\n",
        "  print(i[:-1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "brier_score, hidden layer, number of nodes, learning rate\n",
            "[0.0979749663307075, 2, 2000, 0.001]\n",
            "[0.09969601026343161, 2, 3000, 0.001]\n",
            "[0.10017798924924226, 2, 2000, 0.0001]\n",
            "[0.10088860570725415, 1, 2000, 0.002154434690031894]\n",
            "[0.10198860345920831, 1, 2000, 0.001]\n",
            "[0.10244476416907433, 1, 3000, 0.000849753435908648]\n",
            "[0.10246278171000388, 1, 3000, 0.001]\n",
            "[0.10253665262191089, 2, 2000, 0.0001]\n",
            "[0.10333631141200421, 1, 3000, 0.0001]\n",
            "[0.10576775530568079, 2, 3000, 0.0001]\n",
            "[0.10749559966507849, 1, 2000, 0.0001]\n",
            "[0.10809572611835254, 2, 3000, 0.0001]\n"
          ]
        }
      ]
    }
  ]
}